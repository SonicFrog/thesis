\documentclass[11pt]{book}

\usepackage[english]{babel}
\usepackage{caption}
\usepackage{cite}
\usepackage{color}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage[hidelinks]{hyperref}
\usepackage{listings}
\usepackage{listings-rust}
\usepackage{pdfpages}
\usepackage{todonotes}

\setlength{\textwidth}{146.8mm} % = 210mm - 37mm - 26.2mm
\setlength{\oddsidemargin}{11.6mm} % 37mm - 1in (from hoffset)
\setlength{\evensidemargin}{0.8mm} % = 26.2mm - 1in (from hoffset)
\setlength{\topmargin}{-2.2mm} % = 0mm -1in + 23.2mm
\setlength{\textheight}{221.9mm} % = 297mm -29.5mm -31.6mm - 14mm (12 to accomodate footline with pagenumber)
\setlength{\headheight}{14pt}
\setlength{\parindent}{0pt}

\graphicspath{{../diagram}{../graphs}}

\usepackage{setspace}
\setstretch{1.1}

\makeatletter
\setlength{\@fptop}{0pt}
\makeatother

\usepackage{fancyhdr}
\renewcommand{\sectionmark}[1]{\markright{\thesection\ #1}}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}
\fancyhead[OR]{\bfseries \nouppercase{\rightmark}}
\fancyhead[EL]{\bfseries \nouppercase{\leftmark}}
\fancyfoot[EL,OR]{\thepage}
\fancypagestyle{plain}{
  \fancyhf{}
  \renewcommand{\headrulewidth}{0pt}
  \renewcommand{\footrulewidth}{0pt}
  \fancyfoot[EL,OR]{\thepage}}
\fancypagestyle{addpagenumbersforpdfimports}{
  \fancyhead{}
  \renewcommand{\headrulewidth}{0pt}
  \fancyfoot{}
  \fancyfoot[RO,LE]{\thepage}
}

\usepackage{tikz}
\usepackage[explicit]{titlesec}
\newcommand*\chapterlabel{}
% \renewcommand{\thechapter}{\Roman{chapter}}
\titleformat{\chapter}[display]  % type (section,chapter,etc...) to vary,  shape (eg display-type)
{\normalfont\bfseries\Huge} % format of the chapter
{\gdef\chapterlabel{\thechapter\ }}     % the label
{0pt} % separation between label and chapter-title
{\begin{tikzpicture}[remember picture,overlay]
    \node[yshift=-8cm] at (current page.north west)
    {\begin{tikzpicture}[remember picture, overlay]
        \draw[fill=black] (0,0) rectangle(35.5mm,15mm);
        \node[anchor=north east,yshift=-7.2cm,xshift=34mm,minimum height=30mm,inner sep=0mm] at (current page.north west)
        {\parbox[top][30mm][t]{15mm}{\raggedleft $\phantom{\textrm{l}}$\color{white}\chapterlabel}};  %the black l is just to get better base-line alingement
        \node[anchor=north west,yshift=-7.2cm,xshift=37mm,text width=\textwidth,minimum height=30mm,inner sep=0mm] at (current page.north west)
        {\parbox[top][30mm][t]{\textwidth}{\color{black}#1}};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \gdef\chapterlabel{}
} % code before the title body

\titlespacing*{\chapter}{0pt}{50pt}{30pt}
\titlespacing*{\section}{0pt}{13.2pt}{*0}  % 13.2pt is line spacing for a text with 11pt font size
\titlespacing*{\subsection}{0pt}{13.2pt}{*0}
\titlespacing*{\subsubsection}{0pt}{13.2pt}{*0}

\newcounter{myparts}
\newcommand*\partlabel{}
\titleformat{\part}[display]  % type (section,chapter,etc...) to vary,  shape (eg display-type)
{\normalfont\bfseries\Huge} % format of the part
{\gdef\partlabel{\thepart\ }}     % the label
{0pt} % separation between label and part-title
{\setlength{\unitlength}{20mm}
  \addtocounter{myparts}{1}
  \begin{tikzpicture}[remember picture,overlay]
    \node[anchor=north west,xshift=-65mm,yshift=-6.9cm-\value{myparts}*20mm] at (current page.north east) % for unknown reasons: 3mm missing -> 65 instead of 62
    {\begin{tikzpicture}[remember picture, overlay]
        \draw[fill=black] (0,0) rectangle(62mm,20mm);   % -\value{myparts}\unitlength
        \node[anchor=north west,yshift=-6.1cm-\value{myparts}*20mm,xshift=-60.5mm,minimum height=30mm,inner sep=0mm] at (current page.north east)
        {\parbox[top][30mm][t]{55mm}{\raggedright \color{white}Part \partlabel $\phantom{\textrm{l}}$}};  %the phantom l is just to get better base-line alingement
        \node[anchor=north east,yshift=-6.1cm-\value{myparts}*20mm,xshift=-63.5mm,text width=\textwidth,minimum height=30mm,inner sep=0mm] at (current page.north east)
        {\parbox[top][30mm][t]{\textwidth}{\raggedleft \color{black}#1}};
      \end{tikzpicture}
    };
  \end{tikzpicture}
  \gdef\partlabel{}
} % code before the title body

\usepackage{etoolbox}
\let\bbordermatrix\bordermatrix
\patchcmd{\bbordermatrix}{8.75}{4.75}{}{}
\patchcmd{\bbordermatrix}{\left(}{\left[}{}{}
    \patchcmd{\bbordermatrix}{\right)}{\right]}{}{}
\usepackage{amsmath}
% Fix the problem with delimiter size caused by fourier and amsmath packages.
\makeatletter
\def\resetMathstrut@{%
  \setbox\z@\hbox{%
    \mathchardef\@tempa\mathcode`\(\relax
    \def\@tempb##1"##2##3{\the\textfont"##3\char"}%
    \expandafter\@tempb\meaning\@tempa \relax
  }%
  \ht\Mathstrutbox@1.2\ht\z@ \dp\Mathstrutbox@1.2\dp\z@
}
\makeatother

\lstset{
  backgroundcolor=\color{white},
  basicstyle=\footnotesize,
  breaklines=false,
  deletekeywords= {stack},
  language=Rust,
  frame=single,
  keywordstyle=\color{green},
}

\author{Ogier Bouvier}
\date{\today}
\title{A zero-copy key-value store in Rust}

\begin{document}

\begin{titlepage}
  \newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here
  \center

  \LARGE \textsc{École Polytechnique Fédérale de Lausanne}
  \vspace{1cm}

  \LARGE Master Thesis
  \vspace{2cm}

  {
    \HRule \\[0.5cm]
    \Huge A zero copy key-value store in Rust}
  \HRule \\[1cm]

  \begin{minipage}[t]{0.4\textwidth}
    \large
    {\large \today}
  \end{minipage}\\[1cm]

  \vspace{2cm}

  \begin{tabular}{c c}
    \begin{minipage}[t]{0.4\textwidth}
      \begin{flushleft} \large
        \textbf{Author} \\
        Ogier \textsc{Bouvier}\\
        \textit{ogier.bouvier@epfl.ch}
      \end{flushleft}
    \end{minipage}

    \begin{minipage}[t]{0.4\textwidth}
      \begin{flushright} \large
        \textbf{Supervisors} \\
        Prof. Edouard \textsc{Bugnion}\\
        DCSL | EPFL \\
        \textit{edouard.bugnion@epfl.ch} \\
        Marios \textsc{Kogias}\\
        DCSL | EPFL \\
        \textit{marios.kogias@epfl.ch}
      \end{flushright}
    \end{minipage}
  \end{tabular}

  \begin{center}
    \begin{figure}[h]
      \includegraphics[width=0.71\textwidth]{../diagram/epfl.pdf}
    \end{figure}
  \end{center}

\end{titlepage}

\pagenumbering{gobble}
\tableofcontents

\listoftodos{}

\pagenumbering{arabic}
\setcounter{page}{1}
\chapter{Abstract}

The state of the art today for high-speed networking involves kernel
bypassing techniques making use of software such as DPDK to reduce the
overhead typically associated with kernel based networking. Such
software is usually much faster than traditional kernel networking,
mostly due to avoidance of context switches and data copies between
kernel and user space, especially when processing lots of small
packets which often happens in the case of key-value stores such as
memcached. Nonetheless these systems are far from perfect. First,
having all the  networking code and the user application inside the
same address space means that user code can corrupt the networking
stack very easily. Secondly, development of such applications is also
much more cumbersome than traditional kernel based approaches, indeed
having the user program in the same address space as the networking
code creates the need for it to be aware of low level networking
details that traditional applications don't need to worry about.The
necessity to handle this kind of networking details make kernel bypass
application much more time consuming and error-prone to develop than
regular socket based ones. Another problem these kind of systems
encounter is their inability to ensure safety, i.e.\ that data being
held in the networking stack is not modified by the user program
(which still holds a pointer to it) while for example waiting for a
TCP ACK.

\section{Rust}

We make the claim that Rust~\cite{rustbook} is a better suited
language than C for kernel bypass networked applications. The safety
guarantees Rust brings are very well suited for this kind of
applications, while the performance loss (if any) is negligible. As an
additional benefit Rust allows code to be much more concise and
abstract while drastically cutting down on the memory and concurrency
bug tracking time. Its memory management model will also prove
invaluable when ensuring integrity of data in between the networking
stack and the user application.

As a proof of concept we implement a key-value store on top of
DPDK in order to prove that it is doable as well as reasonable,
performance-wise, to use Rust in this kind of scenario.

\section{Rust}

Rust is a systems programming language from Mozilla Research, stemming
from Mozilla's dissatisfaction with C++. As of today several parts of
Firefox's rendering engine, Servo, have been rewritten in Rust after
the language was deemed mature enough to do so. Rust is engineered to
provide a lot of safety guarantees, notably memory and concurrency
safety through its type system thereby eliminating a lot of common
problems in systems programming.

The strong Rust guarantees also speed up development significantly by
eliminating whole categories of bugs. Most importantly, Rust's type system
eliminates use-after-free, double-free, invalid pointers, memory leaks
and data-races. Overall, the time writing code is increasing since the
compiler will often refuse straight out to compile code that
potentially violates its guarantees. The counter-part to this is that
the time spent debugging is very much reduced. This is particularly
useful when tracking down data race bugs which are notoriously hard to
diagnose. Those kind of bugs are usually found much later in the
development cycle and are usually quite hard to reproduce since they
depend on a certain scheduling of the execution.

\section{Memory safety}

One of the most important feature of Rust is its guarantee of memory
safety, providing both the speed of manual memory management and the
safety of garbage collected languages.

To provide this feature, Rust introduces a new concept called
lifetimes and makes raw pointers second class citizens. These raw
pointers (such as that of C) can't be dereferenced since the compiler
can't guarantee that they are valid. Pointers are hard to validate and
verify statically mostly because of pointer arithmetic. References, on
the other hand, are guaranteed to always point to a valid memory
location. Like the references in C++, references in Rust do not allow
pointer arithmetic, they can only be created from a variable, which
ensures that the referenced memory is valid.

Lifetimes are the central part of Rust's memory safety
guarantees. They work in conjunction with references to ensure
that references are used correctly (i.e.\ not after the value they
refer to has been destroyed), since they allow the compiler to tell
when a reference might outlive the value it refers to. The combination
of lifetimes and references thus eliminate the dangling pointers
problem entirely. Lifetimes are Rust's way of eliminating the need for
garbage collection while not requiring users to manually manage
memory. A lifetime gets attached to each variable created, the
compiler then analyzes the control flow of the program to ensure that
references or values derived from that first value (such as reference
to a struct member) are not used after the value is destroyed.

However Rust is a at the core a systems programming language so we
need some way to bypass the safety restrictions it provides. This is
where the \textbf{unsafe}~\cite{rustonomicon} keyword comes in. This
keyword provides a way to tell the Rust compiler that a piece of code
upholds the memory safety guarantees even though the former can't
verify  statically that it does. Handling of raw pointers and call to
foreign functions are cases where the use of \textbf{unsafe} is
required. Indeed other languages do not provide any of Rust's
guarantees and do not know about lifetimes. This problem also occurs
when developing kernels~\cite{rust-os}~\cite{redox}.

\section{Concurrency safety}

Rust's type system also provides protection against data-races, a
common problem when using C usually solved through atomic operations
and locks. While it is possible to solve this kind of problems in C,
it requires careful design and is an error-prone task. Rust therefore
allows quicker and safer development of concurrent software. This safety
does not come for free though. In order to guarantee that there is no
data-race anywhere, Rust only allows either one mutable reference or
any number of immutable reference to any given variable. By doing
that, the compiler can ensure that the variable will only be read
concurrently or written exclusively.

Even though we speak of concurrency safety, Rust does not provide
protection against deadlocks. This is far too complex a task for
static analysis in a compiler since it requires costly call graph
analysis and sometimes produces false
positives~\cite{deadlock-detection}.

However, the Rust standard library provides safe ways to mutate shared
variables. As an example the RwLock standard library structure allows
creating mutable reference to a value from an immutable RwLock instance.

\section{Performance}

Being a systems programming language, performance is critical to
Rust. All the aforementioned guarantees are enforced statically so
they do not incur any performance penalty. Removing pointer aliasing
from the language also allows the compiler to do many optimizations
that are not feasible safely in C or C++. As to the performance of the
generated assembly code, Rust utilizes the LLVM compiler
infrastructure and thus benefits from all the work that was done
there. Rust also features what the Rust developers call 0-cost
abstraction. Idiomatic Rust makes heavy use of closures, and that
would incur, in some languages, a slight degradation of
performance. But, in Rust, closure can be, most of the time, inlined
thus making as if the programmer wrote the concrete code while
allowing the interface to remain abstract.

In this regard, Rust combines the best of both worlds, high-level
readable and abstract code while also having performance close to that
of other systems programming languages.

\chapter{Design}

\section{Goals} \label{design-goals}

The goal here is to provide a zero-copy, in-memory, eventually
consistent key-value store using kernel bypassing to support high
throughput as well as low latency. The zero-copy property will allow
our latency to remain the same regardless of the sizes of keys and
values, though we can't say the same for the throughput since we will
be limited by the capacity of the NIC in term of raw bandwidth.

Providing low latency and high throughput requires every component of
the system to be fast and zero-copy. We therefore need a fast
networking stack and obviously a high-performance and concurrent
hashmap to store key-value entries as well as an efficient way to
interface with the network interface controller that will send our
packets on the wire. The choice of the networking protocol to transmit
requests and responses is also quite important since it will be the
central part of the system and will determine how much processing is
necessary to receive and send packets.

But creating a fast system is only part of the goal here. The
networking code should be re-usable in order to facilitate further
development of safe high-performance networking
applications in Rust. Providing a convenient and clear API to the
networking stack is therefore also a design goal. That means the API
should follow idiomatic Rust interface design principles and leverage
all the language features necessary to make it re-usable and generic.

\section{Local store} \label{sec:local-store-design}

The central component of any key-value store is an efficient
datastructure to store and retrieve values by their keys. Since we
also aim to provide true zero-copy semantics, this datastructure
should also avoid copying data around. Moreover, it should allow
values to be retrieved and used concurrently with minimum overhead.
A way to make values live outside of the store should also be provided
in order to permit said values to be held in the networking stack
after having been removed from the store.

With all these constraints in mind, we go with the design shown
in~\hyperref[fig:hashmap]{figure~\ref{fig:hashmap}}. The top level
table is a contiguous array of atomic pointers to overflow
buckets. And in turn each overflow bucket is a pointer to a key-value
pair structure allowing fast, easy and zero-copy retrieval of values.

\begin{figure}[htb!]
  \includegraphics[width=\textwidth]{../diagram/amap1.pdf}
  \caption{Hashmap overview}
  \label{fig:hashmap}
\end{figure}

To satisfy the constraints, each pointer inside each overflow bucket,
as well as the pointers to the overflow buckets themselves are
atomically reference counted. Since our key-value store will use many
threads working concurrently to satisfy requests an atomic reference
count is necessary to avoid race conditions when updating the
reference count. But more importantly an atomic reference count allows
values to be removed from the store while still being used elsewhere,
as shown
in~\hyperref[fig:value-sharing]{figure~\ref{fig:value-sharing}}.

\begin{figure}[htb!]
  \includegraphics[width=\textwidth]{../diagram/cc1}
  \caption{Value sharing}
  \label{fig:value-sharing}
\end{figure}

We should also, according to the constraints support lock-free and
fast insertion or replacement of values while not interfering with
concurrent retrievals. This is where making the pointers to the
buckets atomic will come in handy. Insertions can then be done as
follows, atomically retrieve the bucket the value we are inserting
belongs in, increase its reference count to ensure it won't be freed
concurrently. Then create a thread local copy of said bucket. At this
point any concurrent retrieval request can still fetch the shared
value of the bucket, so the insertion does not interfere with the
retrieval. The inserting thread can then make the necessary
modifications to its local copy of the bucket, as shown
in~\ref{fig:omvcc-insert}.

\begin{figure}[htb!]
  \includegraphics[width=\textwidth]{../diagram/amap2.pdf}
  \caption{Value insertion}
  \label{fig:omvcc-insert}
\end{figure}

Once the local work is done, the inserting thread attempts to swap the
value it constructed with the old one. Here, a compare and swap is
necessary, since it is possible than another thread was modifying the
same bucket concurrently and has already swapped the old value with
the one it constructed. To avoid losing updates in the store it is
therefore necessary to reconstruct the thread local value if the
compare and swap was unsuccessful. If the swap was successful on the
other hand, the thread that successfully swap the bucket must now
decrease the reference count of the old bucket. Here, again, the
atomic reference counts make sense since the old bucket could be in
use by any number of other threads performing retrievals in this
bucket. If we consider all the properties that the resulting hashmap
has, we realize that it uses an optimistic multi-version concurrency
control scheme. Optimistic, since each thread works on its own before
attempting to synchronize with other threads, and multi-version since
each bucket can have multiple versions in use at any given point.

\begin{figure}
  \includegraphics[width=\textwidth]{../diagram/amap3.pdf}
  \caption{Bucket swapping}
  \label{fig:omvcc-swap}
\end{figure}

This lock-free design allows a fast, concurrent and zero-copy
insertion of values inside the hashmap. However in the case where the
workload is write-heavy, an optimistic concurrency control scheme will
result in a lot of wasted work as each thread will need to build
the new version of the bucket multiple times. The most expensive
operation for an insert is actually the atomic compare and swap. Since
a typical workload on key-value stores implies a majority of reads
compared to writes (the Facebook USR load is only 0.2\% writes), this
won't be a problem in our target use case.

% rephrase this section
The case shown in~\ref{fig:omvcc} actually showcases how we handle
data shared between the user program and the networking stack. Since
we aim to provide a reliable networking protocol, at some point the
data will be passed to the networking stack for transmission. At this
point the networking stack will hold on to the data until it is
acknowledged by the other end. Our design even allows values to be
removed from the hashmap while still being held in the networking
stack for retransmission, see~\ref{fig:cc2}. The data is then only
referenced in the networking stack, and consequently freed once it is
successfully acknowledged.

\begin{figure}[htb!]
  \includegraphics[width=\textwidth]{../diagram/cc2.pdf}
  \caption{Concurrent value removal}
  \label{fig:cc2}
\end{figure}

On a related note, Rust's concurrency guarantees ensure that data held
in the networking stack can't be modified (since the networking stack
holds an immutable reference to it, there can't be any mutable
reference to that data). The problem of the user program still having
the pointer to the data thus becomes a non-problem since the compiler
ensures that the user program does not modify it.

\section{UDP stack} \label{sec:udp-design}

The core of this project is not the low level details of interfacing
with a NIC or even the kernel bypass. We will thus use an existing
framework to do that for us and build our networking on top of said
framework.

It is worth mentioning the Linux kernel socket flag for zero copy.
Though it does not provide guaranteed zero copy sending/receiving a
port of nginx using it saw a 9\% increase in performance with minimal
code changes. The choice was made not to use it, since it would
eliminate one of the most interesting aspect of the project, the
ability to have a safe userspace networking stack in the same address
space as the user program.

As for the local store, the UDP stack should uphold the zero-copy
property of the system. That means it should interface with the
kernel-bypassing framework and extract data from that framework while
not copying anything.

Kernel bypassing frameworks follow what is known as a run to
completion model, the driver polls the NIC for incoming
packets~\cite{dpdk-pmd} and hands them over to the user application in
batches. This will influence the API design of the UDP stack and
forbid the classic socket style API typically found in kernel
networking stacks. Since we aim to provide a convenient and safe Rust
API to the UDP stack we will need to come up with an alternative
design.

The run to completion model thus prompts a callback based API
design. We thus have a callback oriented UDP stack. Since the
framework polls the NIC for packets on its own this is
appropriate. When the user application creates a new socket on the
stack, it will provide a closure. Once a packet is received, its
headers will be parsed and the UDP stack will dispatch to the
appropriate socket registered earlier by the user application. Of
course the callback provided by the user application should be
reentrant since it will potentially be invoked from many threads
concurrently.

\section{R2P2 stack}

UDP does not provide any kind of reliable delivery of packets. We thus
need some other protocol on top of UDP to provide reliable delivery of
packets. R2P2, the Request Response Pair Protocol, is a highly
scalable RPC protocol. We therefore chose to use a slightly modified
version of R2P2. The published version of R2P2 does not provide
reliable delivery, only load-balancing and request format. We
therefore add explicit acknowledgement of responses from
clients. Since client code and a benchmarking suite  already exist for
R2P2, the choice was made to use it as an RPC protocol for our key
value store.

As stated in the design goals the R2P2 stack should provide an
easy-to-use, reliable and zero-copy interface to user
applications. This R2P2 stack is the interface between the user
application itself (the key-value store in our case) and the UDP
networking stack and handles request parsing, response sending and
retransmissions when needed.

The R2P2 stack will be implemented on top of the UDP stack described in
section~\ref{sec:udp-design}. The API design of the R2P2 stack is
therefore constrained by the design of the UDP stack. We thus make use
of callback in the R2P2 stack. The R2P2 stack itself interfaces with
the UDP stack by registered a callback invoked on each udp
packet. Since R2P2 provides multi-packet requests, the R2P2 should
handle reconstruction of requests. Since the user application will
only care about handling complete requests the R2P2 stack will store
packets and group by them by request and finally invoke the user
provided callback once the whole request has been received and
parsed. This callback should construct the response from the
request. The R2P2 then handles transmitting the response to the client
using the UDP stack.

The R2P2 layer is also responsible for the reliable delivery of
responses. It should then hold on to responses produced by the user
callback until they are acknowledged by the clients. In order to avoid
duplicating work, and thereby reducing both latency and throughput,
it should also identify when a request has already been processed by
the user application and avoid processing this request again.

\section{Key-value store}

The key-value store is what would be considered as the user
application. As such it will provide an example of how to use the
system. It is the last layer of the system and should not rely on any
details of the lower layers of the networking stack. Its role in the
system while be to process individual R2P2 request and parse them
using the application layer protocol.

\chapter{Implementation}

We now discuss how our abstract design translates into a concrete Rust
implementation, including how it ties into the Rust standard library
and precisely how each component interfaces with the others.

\section{Design overview}

Let's start with an overview of the system as a whole before diving
into the details of each individual piece.

\begin{figure}[htb!]
  \includegraphics[width=\textwidth]{../diagram/overview.pdf}
  \caption{System overview}
  \label{fig:design-overview}
\end{figure}

Each packet is first received by DPDK from the NIC\@. The packets are
then processed through the NetBricks pipeline to be handed over to the
UDP stack. The UDP stack then filters and matches packets to a given
socket. Each socket has an associated callback registered by the
user. In our case, the socket are created by the R2P2 server which
uses them to receive R2P2 requests from the network. Once every packet
in a request has been received, the R2P2 server then invokes its own
user callback, this time called the Request Callback. This callback
is registered by the last layer in the system, the actual key-value
store. The key-value store uses R2P2 to satisfy requests in the Redis
protocol. This is also where our hashmap comes in (though it is used
all across the system), to store the key value pairs.

\section{OptiMap} \label{sec:optimap-impl}

OptiMap is the name for the highly concurrent hashmap, described in
section~\ref{sec:local-store-design}, and we now will see how exactly
the design translates into a concrete Rust implementation.

\subsection{AtomicBox}

As stated in section~\ref{sec:local-store-design}, we need some way to
atomically swap values while also atomically reference counting them.
Unfortunately the rust standard library only provides a mean to
atomically swap raw pointers using the \textbf{AtomicPtr}
construct. This effectively forces us to make use of raw pointers,
thus not benefiting from any of Rust's memory safety guarantee and
potentially leading to memory corruption or leaks. Even though we are
forced to use unsafe, Rust still provides strong safety guarantees and
debugging time is still very much reduced since we know for sure that
memory corruption problems are located inside \textbf{unsafe} code
blocks.

\begin{figure}[htb!]
  \lstinputlisting{../code/abox.rs}
  \caption{The AtomicBox structure}
\end{figure}

This leads us to the \textbf{AtomicBox} abstraction, a safe wrapper
around \textbf{AtomicPtr}, that we will use to build our OMVCC
hashmap. The AtomicBox makes use of X86 CAS instructions to improve on
Arc. Arc provides an atomic reference count and AtomicBox provides an
atomic reference count as well the possibility to atomically swap that
value. Since every value is atomically reference counted it will stay
allocated as long as any reference to it still exists while allowing
new requests to fetch the newer value to do their work. Updates are
made using the value atomically fetched at that point in time,
creating a copy, modifying it as appropriate and then swapping it back
with the old one. If the swap succeeds the old value's reference count
is decreased (and dropped if we had the last reference), effectively
providing an optimistic multi-version concurrency control. If the swap
fails, i.e.\ someone already swapped it with another value, we repeat
the same process until the swap is successful.

To build the AtomicBox abstraction we make use of the capability of
Arc to be transformed into raw pointers, thus allowing a user to
control exactly how long the memory on the heap lives, including the
value outliving the AtomicBox instance. The reference can then be
transformed back into an Arc, although this requires using
\textbf{unsafe} since we turn arbitrary pointers into Arc instances
therefore introducing a risk of double-free or invalid memory
accesses.

\begin{figure}[htb!]
  \label{code:atomicbox-interface}
  \lstinputlisting{../code/abox_take.rs}
  \caption{AtomicBox public interface}
\end{figure}

The AtomicBox abstraction allows us to implement the optimistic
multi-version concurrency control scheme. Indeed, the AtomicBox allows
users to extract an Arc to the value referenced by the AtomicPtr it
contains. We can thus extract values from the AtomicBox and keep them
even when they are removed from the AtomicPtr as shown in
figure~\ref{code:atomicbox-interface}.

The public interface provided by AtomicBox aims to be simple, fast and
safe, hiding the unsafe details of the actual operation. We therefore
provide an idiomatic way to update the value as well as a clean way to
fetch the value. Updates are handled through closures, the user passes
a closure to the AtomicBox which will apply this closure to the
current value and then atomically replace the old value with the new
one. In order to allow the value contained in the AtomicBox to outlive
it, when getting the value we actually provide an Arc<T> to the user
making the allocation of the value itself independent from the
allocation of the AtomicBox.

Since they are to be shared amongst threads, all the hashmap
components are allocated on the heap, thus justifying our use of
Arcs. As all elements are of small size (most of them are one or two
pointers big), we make the claim that memory fragmentation is not
gonna be an issue in our case. Indeed most of the allocation with
respect to size will be coming directly from DPDK allocating memory
buffers from Linux's \textbf{hugepages} and thus won't affect the
heap.

\section{Networking}

Let's now dive into the networking layer of the system starting with
the lower layer, that is the interfacing with DPDK.

\subsection{DPDK} \todo{rephrase section}

As you probably know, DPDK is a kernel-bypass networking framework
developed by Intel, typically for high-performance networking
applications. We thus implement all our networking stack on top of
DPDK. It provides drivers for Intel NICs and high-performance packet
management and allocation, as well as lots of networking related
libraries.

DPDK being written in C, we must first find some (ideally, convenient)
bindings allowing us to make calls into DPDK from Rust code. Another
thing to consider is the inherent unsafety in calling C code from
Rust. Since C code has none of the guarantees of Rust calls to C
functions must be in an unsafe block. A convenient safe Rust interface
is therefore a must for speed and ease of development.

\subsection{NetBricks}
NetBricks~\cite{netbricks} is a Rust framework on top of DPDK,
providing a clean and Rust-friendly API, as well as advanced packet
processing facilities. It is, though, aimed at rapid development of
network functions which does not quite fit our current target. A fork
was therefore necessary to modify it to suit the particular needs of a
host networking framework. An example of the NetBricks packet
processing is shown in figure~\ref{code:udp-pipeline}.

\begin{figure}[htb!]
\begin{lstlisting}
ReceiveBatch::new(queue.clone())
    .parse::<MacHeader>()
    .parse::<IpHeader>()
    .metadata(box move |pkt|{
        // keep the source ip around for later replies
        pkt.get_header().src()
    })
    .parse::<UdpHeader>()
    .map(box move |pkt| {
        let src_addr = Ipv4Addr::from(*pkt.read_metadata());
        let src_port = pkt.get_header().dst_port();

        stack.sockets.get(&src_port).map(|sock| {
             sock.deliver(&pkt, src_addr, src_port)
        });
    })
    .compose()
\end{lstlisting}

  \label{code:udp-pipeline}
  \caption{UDP packet pipeline}
\end{figure}

In order to adapt NetBricks to end-host networking, the fork comes
with a few modifications in packet management. The first one is the
inability of packets to cross thread boundaries. Indeed since
NetBricks is geared towards network functions the packets are only
meant to be processed by the user defined pipeline and be freed as
soon as they have gone through the pipeline. To remedy this we create
the CrossPacket abstraction which can be created from the standard
NetBricks Packet and is able to cross thread boundaries by being
immutable. But we still need to be able to send packets through
NetBricks and NetBricks does not know how to handle CrossPackets. It
is therefore possible to convert a CrossPacket to a Packet as well as
the reverse.

\begin{figure}[htb!]
  \lstinputlisting{../code/cp.rs}
  \caption{CrossPacket abstraction}
  \label{code:crosspacket}
\end{figure}

The CrossPacket abstraction is the way we work within the limits of
the Rust type system. NetBricks' \textbf{Packet}s are, by default,
mutable meaning the Rust compiler will not allow them to cross thread
boundaries in order to prevent data races. However since the
pointer to the mbuf is immutable this poses a problem when sending the
packet. Indeed DPDK stores headers in the same buffer as the payload,
meaning we can't easily send the same payload concurrently to two
different destination. The way to solve this is make use of DPDK's
mbuf chaining, the first mbuf in the chain being local to the current
thread (therefore being mutable) and the second one being the
immutable payload shared between threads. This is the second notable
modification we make to our NetBricks fork, the ability to chain mbufs
to maintain the immutability of the packets we received from the
network while also having the possibility to send them concurrently to
different destinations.

One more thing that needs to be implemented in NetBricks is packets
living longer than the packet processing pipeline. Since NetBricks is
aimed at network function once a packet has gone through the packet
processing pipeline it has no reason to stay allocated (as in the case
of a firewall, when the decision has been made to forward the packet
it's not needed anymore). But in our case packets are longer lived, we
need to keep them in the store to answer queries later on. We then
need a way to prevent deallocation of packets once they leave the
pipeline. We make use of mbuf reference counts to do this. In our
segmented packets the headers always have a reference count of one,
indeed they are not needed once the packet has been sent successfully,
thus we let DPDK free them once the packet have gone through the
NIC\@. The payload on the other hand should not be deallocated after
sending, so we set its reference count to 2, thereby preventing DPDK
from freeing it. But we still need to free them once they are not
needed anymore. The problem here is that mbuf reference counts are not
atomic, this is a problem in our case since the same payload could be
sent from two different threads which will both need to increment
it. That means we need to wrap the actual payload and provide an
atomic reference counting capability to prevent mbufs from beeing
freed too early.

\subsection{UDP stack}

The UDP stack ties in to NetBricks by registering a pipeline on all
available cores and using each pipeline to dispatch packets to the
correct socket. Each socket is stored in a hashmap in the stack by its
port. Upon receiving a packet the stack filters in multiple steps the
packets dropping invalid ones and routing the others to the correct
socket. The corresponding callback is then called on the packets one
by one.

\begin{figure}[htb!]
  \lstinputlisting{../code/udp-sock.rs}
  \label{code:socket-registration}
  \caption{UDP stack public interface}
\end{figure}

The bind method shown in figure~\ref{code:socket-registration} allows
users to bind socket on a given UdpStack. The user provides the IP
address to listen on as well as the UDP port. In order to receive
incoming packets the user also provides a closure that will be called
every time a packet is read from the NIC by the UdpStack. The stack
will actually transfer ownership of the packets received to the
callback, thereby permitting the zero copy networking stack we are
aiming for.

The send method on the other hand does not need to transfer ownership
of the packets to the UDP stack, because it does not need to keep the
packets after sending them. Therefore it only borrows the Packet the
user wants to send. After which it creates a segmented DPDK packet to
allow sending the same packet concurrently to different destinations.
To avoid flooding the same RX ring, send() uses a simple round-robin
approach to select the output queue.

\subsection{R2P2 server} \todo{rephrase section}

We use a variant of the R2P2 protocol that adds active acknowledgement
from clients in order to reduce latency when packets are dropped. For
the sake of simplicity we also do not handle packets that are
delivered out of order, mostly to avoid resizing buffers used to store
the request while it is not completely received.

The R2P2 stack is built on top of the aforementioned UDP stack. The
R2P2 server registers a packet callback and then dispatches the packet
to the correct request after parsing the headers. Again the R2P2
server uses callbacks registered by the user application to handle
requests. These callbacks return a R2P2Response structure that is then
sent by the R2P2 server through the UDP stack.

The R2P2 server handles reliable delivery through client
acknowledgement. The response returned by the user callback is stored
in the R2P2 server while waiting for the acknowledgement and
retransmitted if need be.

\subsection{Key value store}

The key-value store uses a simplified version of the Redis
protocol. It only supports SET and GET requests, since the test
workload has no other type of requests.

\todo{Describe how we build keys and values from the raw Packets}

It then handles construction of the actual value that will be stored
in the hashmap. The requirements for the actual hashmap value and key
datastructures are mostly to play nice with the Rust memory management
model and DPDK mbufs. Indeed since we use segmented packets, we need
the headers to be freed once the request has been satisfied but the
payload itself must be kept as long as either the hashmap or the
networking hold it. As the payload can be sent concurrently from
different threads we need some sort of atomic reference counting on
the mbufs. The problem here is that mbufs reference counts are not
atomic (they're plain 16 bits integer).

\chapter{Evaluation}
\label{chap:evaluation}

We will now evaluate the performance of the system as well as the
performance of the local store.

\todo{Describe test machines}

\section{Local store}
\label{sec:eval-local}

We first evaluate how the local hashmap performance, especially when
under a lot of load. We use UTF-8 string keys of 128 bytes with values
of 64 bits to simulate only copying pointers to packets from the
network. We generate a random workload for each of the 15 concurrent
thread and start them simultaneously. Each thread performs one million
GET and 1 million PUT request, on a previously initialized map.

\subsection{No collisions, balanced load}

We first establish the optimal performance we can expect from the
hashmap by reducing the contention on the buckets, thus reducing the
amount of wasted work by threads trying to swap out the buckets
unsuccessfully. To this end, we allocate four times more buckets than
they are keys and using a uniform distribution when generating
workload. The set of keys is pre-determined and no new keys are
inserted throughout the test, meaning the size of each bucket will
stay constant.

\begin{center}
  \begin{tabular}{c c} \label{table:nocol-balanced}
    \includegraphics[width=0.5\textwidth]{../csv/balanced_no_collision_PUT.pdf}
    &
      \includegraphics[width=0.5\textwidth]{../csv/balanced_no_collision_GET.pdf}
  \end{tabular}
  \captionof{table}{Distribution of request durations}
\end{center}

\subsection{Artificial collision, balanced load}

OMVCC scheme are known for performing badly under write heavy
workloads. So we now consider the case of a skewed and write heavy
workload and evaluate the performance of the hashmap in this
scenario. In order to simulate a skewed workload, we reduce the number
of buckets in the hashmap. This will create lots of contention between
threads for each buckets since the chance of two keys mapping to the
same bucket is increased by a factor of 4. The test setup is, again,
the same as described in section~\ref{sec:eval-local}.

\begin{center}
  \begin{tabular}{c c} \label{table:col-balanced}
    \includegraphics[width=0.5\textwidth]{../csv/balanced_collision_PUT.pdf}
    &
      \includegraphics[width=0.5\textwidth]{../csv/balanced_collision_GET.pdf}
  \end{tabular}
  \captionof{table}{Distribution of request durations with collision}
\end{center}
\todo{More test setups?}

\section{Networked store}

\missingfigure{Throughput / latency graph}
\missingfigure{Latency histogram}

\chapter{Conclusion}

We hope to have provided an easy to use and clear API to build upon
our system. We also think that we have proved that Rust is a valid
language to build kernel bypass networking in.

\section{Further work}

We finish by mentioning what features are missing or could be useful
in such a piece of software. We have provided a UDP stack on top of
NetBricks, even though in real-life scenario of networking
applications UDP is rarely used. A TCP stack on top of NetBricks would
therefore be a considerable improvement.

One other thing that makes sense to have is an ARP handler. In our
case we respond to a request, so we have both the source and
destination MAC addresses at hand. But in the case where we want to
send a packet to an arbitrary IP we need to figure out the destination
MAC address. This is where an ARP protocol handler would come in
handy. \todo{IPv6}

With an ARP and TCP stack we would be close to a full featured
networking framework in Rust for kernel bypass. Such a system would
provide both the speed and the convenience of traditional socket based
networking while also providing better networking performance and the
safety of the Rust programming language. \todo{complete kernel bypass
 networking framework in Rust}

\newpage
\pagenumbering{gobble}
\bibliographystyle{plain}
\bibliography{master}{}

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
