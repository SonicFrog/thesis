\documentclass[11pt]{article}

\usepackage{color}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{pdfpages}

\graphicspath{{../diagram}{../graphs}}

\author{Ogier Bouvier}
\date{\today}
\title{A zero-copy key-value store in Rust}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Abstract}
State-of-the-art today for high-speed networking involves kernel
bypassing techniques making use of software such as DPDK to reduce the
overhead typically associated with kernel based networking. Such
software usually is much faster than traditional kernel networking,
mostly due to avoidance of context switches between kernel and
user space, especially when processing lots of small packets which
happens often in the case of key-value stores such as memcached.
Nonetheless these systems are far from perfect, having all the
networking code and the user application inside the same address
space means that user code can corrupt the networking stack very
easily. Another problem these kind of systems encounter is their
inability to ensure safety, i.e.\ that data being held in the
networking stack is not modified by the user program (which still
holds a pointer to it) while for example waiting for a TCP ACK.

\subsection{Rust}
We propose that Rust is a better suited language than C for kernel
bypass networked applications. The safety guarantees Rust
brings are very well suited for this kind of applications, while the
performance loss (if any) is negligible. As an additional benefit Rust
allows code to be much more concise and abstract while drastically
cutting down on the memory and concurrency bug tracking time.

As a proof-of-concept we implement a key-value store on top of
NetBricks in order to prove that it is doable as well as reasonable
performance-wise to use Rust in this kind of scenario.

\section{Rust}
Rust is a systems programming language from Mozilla Research, stemming
from Mozilla's dissatisfaction with C++. As of today several parts of
Firefox's rendering engine, Mozilla's web browser, have been rewritten
in  Rust after the language was deemed mature enough to do so. Rust is
engineered to  provide a lot of safety guarantees, notably memory and
concurrency safety, through its type system thereby eliminating a lot
of common problems in systems programming. Most notably, Rust's type
system eliminates dangling pointers, memory leaks and data-races.

The strong rust guarantees also speeds up development significantly by
eliminating whole categories of bugs. On that note, the Rust type
system exposed a bug in the design of the store. Indeed DPDK stores
packet headers in the same buffer as the payload thus preventing
sharing the packet buffer across threads. In C this is the kind of
bugs that takes lots of effort and time to locate and resolve, but
that is, with the Rust guarantees, taken care of by the compiler and
easily located.

\subsection{Memory safety}

Rust ensures memory safety through the use of reference and
lifetimes. Raw pointers (such as that of C) can't be dereferenced
except through the use of the \textbf{unsafe} keyword. References are
guaranteed to always point to a valid memory location. Lifetimes are
critical to ensuring that references are always valid, since they
allow the compiler to tell when a reference might outlive the value it
refers to. The combination of lifetimes and references thus eliminate
the dangling pointers problem entirely as long as we don't use the
\textbf{unsafe} feature of Rust.

Lifetimes are also part of Rust's memory safety guarantees. Lifetimes
are Rust's way of eliminating the need for garbage collection while
not requiring users to manually manage memory. A lifetime gets
attached to each variable created, the compiler then analyzes the
control flow of the program to ensure that references or values
derived from that first value (such as reference to a struct member)
are not used after the value is destroyed.

\subsection{Concurrency safety}
Rust also provides protection against data-races, a common problem
when using C usually solved through atomic operations and locks.
While it is possible to solve this kind of problems in C, it requires
careful design and is an error-prone task. Rust therefore allows for
quicker and safer concurrent software development. This safety does not
come for free though. In order to guarantee that there is no data-race
anywhere, Rust only allows either one mutable reference or any number
of immutable reference to any given variable. By doing that, the
compiler can ensure that the variable will only be read concurrently
or written exclusively. Safe ways to mutate shared are also included
in Rust. As an example the RwLock standard library structure allows
creating mutable reference to a value from an immutable RwLock
instance.

\subsection{Performance}
Though providing lots of guarantees Rust actually has performance
close (if not superior) to that of C. The Rust compiler makes use of
the LLVM compiler infrastructure and therefore benefits from all the
work done on the optimizer in the LLVM pipeline.

Rustc also does some optimization of its own. The use of closures is very
idiomatic in Rust. Usually (mostly in garbage collected languages)
closure incur a pretty high performance cost, since they must be
allocated on the heap and therefore are more expensive to call than
regular functions. In Rust however, closures are most of the time
inlined, and for generic functions the compiler can even specialize
generic functions and generate code for each variant of the closure
passed to the function.

Rust also features what the Rust developers call 0-cost abstraction,
the closure inlining we talk about above is an example of 0-cost
abstraction, an abstraction that simplifies and generalizes the code
while incurring no runtime performance cost.

\section{Design}

\subsection{Goals}
In order to be truly high-performance, the store should ideally
support fast, lock-free and zero-copy insertion and retrieval. That
means we need to come up with a concurrency control scheme to support
concurrent lock-free updates. Since we don't need to provide strong
consistency guarantees, we can rely on a optimistic multi version
concurrency control scheme for simplicity of implementation and
speed.

\subsection{Local store}
The first step in providing an efficient key-value store is to
have an efficient hashmap to store and retrieve key value pairs
rapidly once the request have been parsed. This also poses the
additional challenge of working within the limit of Rust's borrow
checker and smart use of unsafe. Care was also taken to ensure that
the resulting hashmap is suitable for zero-copy insertion and
retrieval. This means that the ownership of values inside the hashmap
has to be carefully managed to ensure client can retrieve and use
values from the hashmap concurrently with the hashmap being modified.

We go with the following design. The top level table is a contiguous
array of atomic pointers to overflow buckets. And in turn each
overflow bucket is a pointer to a key-value pair structure allowing
easy extraction of both keys and values, as show in \ref{fig:hashmap}.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{../diagram/amap1}
  \caption{Hashmap overview}
  \label{fig:hashmap}
\end{figure}

The pointers inside each overflow buckets are atomically reference
counted for multiple reasons. The first obvious one is that values
live for an undetermined amount of time and are shared between
multiple threads. The second reason is that the pointer to the data
does will not only reside in the hashmap itself but also in the
networking stack while waiting for an ACK, as shown in
\ref{fig:omvcc}.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{../diagram/cc1}
  \caption{Optimistic multi-version concurrency control}
  \label{fig:omvcc}
\end{figure}

Let's know see how we are going to perform updates in the store.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{../diagram/amap2}
  \caption{Value insertion}
  \label{fig:omvcc-insert}
\end{figure}

We first copy the entire overflow bucket, as shown in
~\ref{fig:omvcc-insert}. Since the overflow bucket is only made out of
pointers this is not a costly operation. We also take care to increase
the reference count of each pointer in the overflow bucket to avoid
early frees of the values. Once we have our copy we build the new
version of the overflow bucket locally. Once we have built the new
version we attempt to swap it atomically inside the top-level
table, ~\ref{fig:omvcc-swap}. If the compare and swap fails, we simply
retry by fetching the new value and constructing our own once
again. In the case where the swap is successful, the now-old value of
the bucket may be in use by some concurrent read or write operation,
we thus can't free it right away. This is where the choice of
atomically reference counted pointers comes in handy. We just have to
decrease the reference count of the old bucket, freeing it if we had
the last reference to it.

\begin{figure}
  \includegraphics[width=\textwidth]{../diagram/amap3}
  \caption{Bucket swapping}
  \label{fig:omvcc-swap}
\end{figure}

This lock-free design allows a fast and zero-copy insertion of values
inside the hashmap. The most expensive operation for an insert is
actually the atomic compare and swap. This can become a problem if
there is a lot of threads competing for the same bucket concurrently
since atomic operations can cause stalling. However this would be a
problem only if the workload we are serving is a skewed write-heavy
workload. Since a typical workload on key-value stores implies a
majority of reads compared to writes (the Facebook USR load is only
0.2\% writes), this won't be a problem in our target use case.

The case shown in ~\ref{fig:omvcc} actually showcases how we handle
data shared between the user program and the networking stack. Since
we aim to provide a reliable networking protocol, at some point the
data will be passed to the networking stack for transmission. At this
point the networking stack will hold on to the data until it is
acknowledged by the other end. Our design even allows values to be
removed from the hashmap while still being held in the networking
stack for retransmission, see~\ref{fig:cc2}. The data is then only
referenced in the networking stack, and freed once it is successfully
acknowledged.

\begin{figure}[h!]
  \includegraphics[width=\textwidth]{../diagram/cc2}
  \caption{Concurrent value removal}
\end{figure}

On another note, Rust's concurrency guarantees ensure that data held
in the networking stack can't be modified (since the networking stack
holds an immutable reference to it, there can't be any mutable
reference to that data). The problem of the user program still having
the pointer to the data thus becomes a non-problem since the compiler
ensures that the user program does not modify it.

\subsection{Network considerations}

One thing we need to consider also is the type of values that are
gonna be stored in the hashmap. Indeed, the key and values of the
hashmap are, in the end, going to be coming from the network. The
actual key-value datastructure used is thus critically important,
since it will be the interface for manipulating packet data in a
zero-copy fashion.

One thing that will also be critical is header formatting when sending
a packet out on the network.

\section{Implementation}

\subsection{AtomicBox}

In order to build a hashmap using optimistic multi version concurrency
control we first need a way to atomically swap pointers in a safe
way. Unfortunately the rust standard library only provides a mean to
atomically swap raw pointers using the \textbf{AtomicPtr}
construct. This effectively forces us to make use of raw pointers,
thus not benefiting from any of Rust's memory safety guarantee.

This leads to the \textbf{AtomicBox} abstraction, a safe wrapper
around \textbf{AtomicPtr}, that we use to build our OMVCC hashmap.
The AtomicBox makes use of X86 CAS instructions to improve on Arc.
Arc provides an atomic reference count and AtomicBox provides an
atomic reference count as well the possibility to atomically swap that
value. Since every value is atomically reference counted it will stay
allocated as long as any reference to it still exists while allowing
new requests to fetch the newer value to do their work. Updates are
made using the value atomically fetched at that point in time,
creating a copy, modifying it as appropriate and then swapping it back
with the old one. If the swap succeeds the old value's reference count
is decreased (and dropped if we had the last reference), effectively
providing an optimistic multi-version concurrency control. If the swap
fails, i.e\. someone already swapped it with another value, we repeat
the same process until the swap is successful.

\subsection{Concurrency control}
A more niche feature allows an Arc to be transformed into a
raw pointer while destroying the Arc instance itself, thus allowing
a user to control exactly how long the memory on the heap lives.
The reference can then be transformed back into an Arc, though since
we make use of raw pointers this requires using \textbf{unsafe}.

We can then use this in conjunction with AtomicPtr to provide a
lock-free wrapper allowing us to atomically modify any type of value.
Building on this wrapper we can then implement an AtomicBucket for the
hashmap.

\subsection{Memory management}
All the hashmap components are allocated on the heap. Since they are
all in the same size range (most of them consists only of one or two
pointers), we make the claim that memory fragmentation is not gonna be
an issue in our case. Indeed most of the heavy allocation are coming
from DPDK allocating memory buffers from Linux's \textbf{hugepages}
and thus won't affect the heap.

\section{Networking}

In this section we will delve in the networking part of the key-value
store, highlighting the Rust-DPDK interfacing. We will also talk about
the challenges of providing a true zero-copy networking stack in Rust
on top of DPDK.

\subsection{DPDK}

As you probably know, DPDK is a kernel-bypass networking framework
developed by Intel, typically used for high-performance networking
applications. To support true zero-copy we are required to use some
form of kernel bypassing since the kernel networking is absolutely
not zero-copy. We thus implement all our networking on top of DPDK.
% mention the ZEROCOPY kernel socket flag? though it does *not*
% guarantee zero copy

Since DPDK is written in C, we must first find some (ideally,
convenient) bindings allowing us to make calls into DPDK from Rust
code.

\subsection{NetBricks}
NetBricks is a Rust framework on top of DPDK, providing a clean and
Rust-friendly API, as well as advanced packet processing facilities.
It is, though, aimed at rapid development of network functions which
does not quite fit our current target. A fork was therefore necessary
to modify it to suit the particular needs of a host networking
framework.

\subsection{UDP stack}
The first step in the development of the key value store is obviously
to have some networking support. The choice of UDP makes sense in term
of ease and speed of development given the time constraints of the
project. Moreover, as we will see later on, the use of R2P2 (which
uses UDP as a transport protocol) as an RPC protocol makes a
working UDP stack a requirement.

\subsection{R2P2 server}
R2P2, the Request Response Pair Protocol, is a recent RPC protocol
from EPFL's Datacenter Systems Laboratory. To allow for rapid testing
and benchmarking, the choice was made to use R2P2 as an RPC protocol
for our key value store.

The R2P2 stack is built on top of the aforementioned UDP stack. The
R2P2 server registers a packet callback and then dispatches the packet
to the correct request after parsing the headers.

\subsection{Key value store}

As expected, the key value store itself comes on top of the R2P2
stack. Each R2P2 request received is either a GET, PUT or DELETE
request. The store supports arbitrary key and value size and types,
though it does no validation of any kind on either the keys or values
it is provided.

\subsubsection{Header formatting}

One obvious issue when concurrently satisfying a request for the same
key is headers. DPDK stores the headers in the same buffer as the
packet payload. Rust, for reasons explained above, doesn't allow
mutable pointers to be shared between threads. This means we either
have to lock the buffer to write the headers then send, or use scatter
gather lists to allow the headers and packet payload to be in separate
mbufs.

\section{Evaluation}

We now discuss the performance of our key-value against the USR
Facebook workload.

TODO: describe test machines

\subsection{Local store}

\subsubsection{No collisions}

TODO describe test for no hash collision

\subsubsection{Artificial collision}

TODO describe test setup for hash collision

\subsection{Networked store}

TODO once the R2P2 stack is working

\section{Conclusion}



\subsection{Further work}
ARP layer in the networking stack?
TCP stack

\end{document}
%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
